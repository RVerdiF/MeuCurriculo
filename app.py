import streamlit as st
import plotly.express as px
import pandas as pd
from streamlit_lottie import st_lottie

# --- PAGE CONFIGURATION ---
st.set_page_config(layout="wide", page_title="Portfolio | Rafael Verdi de Freitas")

# --- HTML & JS for Vanta.js Background ---
# This block injects the necessary scripts and styles for the animated background.
vanta_html = """
<style>
#vanta-bg {
    position: fixed;
    top: 0;
    left: 0;
    width: 100vw;
    height: 100vh;
    z-index: -1;
}
</style>
<div id="vanta-bg"></div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r134/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/vanta@latest/dist/vanta.birds.min.js"></script>
<script>
console.log("Vanta script starting");
document.addEventListener("DOMContentLoaded", function() {
    console.log("DOMContentLoaded event fired");
    try {
        VANTA.BIRDS({
          el: "#vanta-bg",
          mouseControls: true,
          touchControls: true,
          gyroControls: false,
          minHeight: 200.00,
          minWidth: 200.00,
          scale: 1.00,
          scaleMobile: 1.00,
          backgroundColor: 0x121212,
          color1: 0x4caf50,
          color2: 0x4caf50,
          colorMode: "lerp"
        });
        console.log("Vanta script executed successfully");
    } catch (e) {
        console.error("Vanta script error:", e);
    }
});
</script>
"""
st.markdown(vanta_html, unsafe_allow_html=True)

# --- Function to load local CSS file ---
def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# --- Function to load Lottie animation from URL ---
def load_lottieurl(url: str):
    return url

# --- ASSETS ---
local_css("style.css")
lottie_animation_url = "https://assets9.lottiefiles.com/packages/lf20_v9riyrep.json"

# --- PDF CV for Download ---
with open("CV.pdf", "rb") as pdf_file:
    PDFbyte = pdf_file.read()

# --- HEADER & INTRO ---
with st.container():
    col1, col2 = st.columns((3, 1))
    with col1:
        st.title("Rafael Verdi de Freitas")
        st.subheader("Data Scientist | Machine Learning Engineer | Fraud Prevention Specialist")
        st.markdown("""
        <a href="https://www.linkedin.com/in/rafael-verdi-de-freitas/" target="_blank" style="text-decoration: none; color: #4CAF50; margin-right: 15px;">LinkedIn</a> 
        <a href="https://github.com/RVerdiF" target="_blank" style="text-decoration: none; color: #4CAF50;">GitHub</a>
        """, unsafe_allow_html=True)
        st.write(" ") 
        st.download_button(label="üìÑ Download CV", data=PDFbyte, file_name="RafaelVerdiFreitas_CV.pdf", mime="application/octet-stream")
    with col2:
        st.image("1594050442709.jpeg", width=230)

st.markdown("---")

# --- TAB CREATION ---
tab1, tab2, tab3, tab4 = st.tabs(["Summary & Skills", "Projects", "Professional Experience", "Education & Qualifications"])

# --- TAB 1: SUMMARY & SKILLS ---
with tab1:
    col1, col2 = st.columns((2, 1))
    with col1:
        st.header("Professional Summary")
        st.write("""
        Results-driven Data Scientist with over 5 years of experience in the financial industry, specializing in Strategic Data Management, Fraud Prevention, and Machine Learning. Proven track record of developing advanced fraud detection algorithms, building predictive models, and enhancing operational efficiency through automation and DevOps practices. A collaborative team player with a strong background in Business Administration, skilled in translating complex data into actionable business insights and driving organizational success.
        """)
    with col2:
        st_lottie(load_lottieurl(lottie_animation_url), speed=1, height=250, key="initial")

    st.markdown("---")
    st.header("Technical Skills")
    skills_data = {
        'Skill': [
            'Python', 'SQL', 'Power BI', 'Predictive Modeling',
            'Snowflake', 'DBT', 'ETL Processes', 'Deep Learning',
            'Apache Spark', 'Docker', 'CI/CD', 'AWS (S3, SageMaker)', 'Kubernetes',
            'Kafka', 'Hadoop', 'NLP', 'MLOps', 'Data Governance', 'Agile', 'Generative AI'
        ],
        'Years': [
            5, 5, 5, 5,
            5, 5, 5, 3,
            2, 2, 3, 3, 1,
            2, 2, 3, 3, 4, 5, 1
        ]
    }
    df_skills = pd.DataFrame(skills_data).sort_values(by="Years", ascending=True)

    fig = px.bar(df_skills, 
                 x='Years', 
                 y='Skill', 
                 orientation='h', 
                 title='Years of Experience',
                 template='plotly_dark',
                 color='Years',
                 color_continuous_scale=px.colors.sequential.Greens_r)
    
    fig.update_layout(
        height=700,
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(title='Years'),
        yaxis=dict(title=''),
        coloraxis_showscale=False,
        title_x=0.5
    )
    st.plotly_chart(fig, use_container_width=True)

# --- TAB 2: PROJECTS ---
with tab2:
    st.header("Projects")
    st.write("Here are some of my projects on GitHub. Click on a title to see the repository!")
    st.markdown("---")

    projects_data = [
        {
            "url": "https://github.com/RVerdiF/api-embrapa-tech-challenge",
            "title": "API Embrapa - Vitivinicultura",
            "readme": '''
API para extra√ß√£o e consulta de informa√ß√µes referentes √† vitivinicultura, baseada em dados da Embrapa.

## Descri√ß√£o

Este projeto consiste em uma API RESTful desenvolvida com FastAPI que fornece acesso a dados sobre vitivinicultura. A API extrai dados do portal Vitibrasil da Embrapa e os disponibiliza em formato JSON atrav√©s de endpoints estruturados.

## Fluxograma geral do Projeto

![Fluxograma da API Embrapa](assets/fluxograma_api.png)

## Diagrama de sequencia do Projeto

![Diagrama de sequencia da API Embrapa](assets/diagrama_de_sequencia.png)

## Categorias de Dados

A API fornece informa√ß√µes sobre:

- **Produ√ß√£o**: Dados sobre a produ√ß√£o de uvas e derivados
- **Comercializa√ß√£o**: Informa√ß√µes sobre a comercializa√ß√£o de produtos vitivin√≠colas
- **Processamento**: Dados sobre o processamento de uvas (vin√≠feras, americanas, mesa e outros)
- **Exporta√ß√£o**: Estat√≠sticas de exporta√ß√£o (vinhos de mesa, espumantes, uvas frescas e sucos)
- **Importa√ß√£o**: Estat√≠sticas de importa√ß√£o (vinhos de mesa, espumantes, uvas frescas, passas e sucos)

## Tecnologias Utilizadas

- **FastAPI**: Framework web para constru√ß√£o de APIs
- **Docker**: Containeriza√ß√£o da aplica√ß√£o
- **Pandas**: Manipula√ß√£o e an√°lise de dados
- **Pydantic**: Valida√ß√£o de dados
- **PyYAML**: Processamento de arquivos YAML para configura√ß√£o

## Requisitos

- Docker e Docker Compose
- Python 3.10+ (para desenvolvimento local)

## Instala√ß√£o e Execu√ß√£o

### Usando Docker (Recomendado)

1. Clone o reposit√≥rio:
   ```
   git clone <url-do-repositorio>
   cd embrapa-api
   ```

2. Crie um arquivo `.env` baseado no `.env.example`:
   ```
   cp .env.example .env
   ```

3. Para ambiente de desenvolvimento (com hot-reload):
   ```
   ./start-dev.bat
   ```
   ou
   ```
   docker-compose -f docker-compose.dev.yml up --build
   ```

4. Para ambiente de produ√ß√£o:
   ```
   ./start-prod.bat
   ```
   ou
   ```
   docker-compose up --build
   ```

### Instala√ß√£o Local (Desenvolvimento)

1. Clone o reposit√≥rio:
   ```
   git clone <url-do-repositorio>
   cd embrapa-api
   ```

2. Execute o script de instala√ß√£o e inicializa√ß√£o:
   ```
   main.bat
   ```

   O script ir√° automaticamente criar um ambiente virtual, instalar as depend√™ncias necess√°rias e iniciar a aplica√ß√£o.

## Uso da API

A API est√° dispon√≠vel em:
- Produ√ß√£o: `https://api-embrapa-tech-challenge.onrender.com/`
- Desenvolvimento local: `http://localhost:8000`


### Documenta√ß√£o Swagger

Acesse a documenta√ß√£o interativa da API em:
- `http://localhost:8000/docs`
- `https://api-embrapa-tech-challenge.onrender.com/docs`

### Endpoints Principais

- **GET /v1/producao**: Retorna dados de produ√ß√£o
- **GET /v1/comercializacao**: Retorna dados de comercializa√ß√£o
- **GET /v1/processamento**: Retorna dados de processamento
- **GET /v1/exportacao**: Retorna dados de exporta√ß√£o
- **GET /v1/importacao**: Retorna dados de importa√ß√£o

### Exemplos de Filtros

Cada categoria possui endpoints para filtrar dados:

- **Por categoria**: `/v1/producao/categoria/{categoria}`
- **Por produto**: `/v1/producao/produto/{produto}`
- **Por ano**: `/v1/producao/ano/{ano}`
- **Por quantidade m√≠nima**: `/v1/producao/quantidade/min/{quantidade}`
- **Por quantidade m√°xima**: `/v1/producao/quantidade/max/{quantidade}`
- **Filtros combinados**: `/v1/producao/filter?categoria=X&produto=Y&ano=2022`

## Estrutura do Projeto

```
./
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Modelos de dados Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ routers/           # Rotas da API
‚îÇ   ‚îú‚îÄ‚îÄ scrapping/         # M√≥dulos para extra√ß√£o de dados
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ docker-compose.dev.yml # Configura√ß√£o Docker para desenvolvimento
‚îú‚îÄ‚îÄ docker-compose.yml     # Configura√ß√£o Docker para produ√ß√£o
‚îú‚îÄ‚îÄ Dockerfile             # Defini√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ main.bat               # Script para execu√ß√£o local em Windows
‚îú‚îÄ‚îÄ main.py                # Ponto de entrada da aplica√ß√£o
‚îú‚îÄ‚îÄ start-dev.bat          # Script para iniciar ambiente de desenvolvimento
‚îú‚îÄ‚îÄ start-prod.bat         # Script para iniciar ambiente de produ√ß√£o
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md              # Este arquivo
```

## Fonte dos Dados

Os dados s√£o extra√≠dos do portal Vitibrasil da Embrapa:
- [http://vitibrasil.cnpuv.embrapa.br/](http://vitibrasil.cnpuv.embrapa.br/)

## Deploy em M√°quina Virtual

### Pr√©-requisitos
- Uma m√°quina virtual com Linux
- Docker e Docker Compose instalados
- Acesso SSH √† m√°quina virtual

### Passo a Passo Simplificado

1. **Conecte-se √† VM e instale Docker**:
   ```bash
   ssh usuario@endereco-da-vm
   sudo apt update && sudo apt install -y docker.io docker-compose
   sudo systemctl enable docker && sudo systemctl start docker
   ```

2. **Clone e execute a aplica√ß√£o**:
   ```bash
   git clone <url-do-repositorio> && cd embrapa-api
   cp .env.example .env
   docker-compose up -d
   ```

3. **Configure um proxy reverso (opcional)**:
   ```bash
   sudo apt install -y nginx
   sudo nano /etc/nginx/sites-available/embrapa-api
   # Adicione a configura√ß√£o b√°sica do Nginx
   sudo ln -s /etc/nginx/sites-available/embrapa-api /etc/nginx/sites-enabled/
   sudo systemctl restart nginx
   ```

## Exemplo de Uso para Machine Learning

### Caso de Uso: Previs√£o de Produ√ß√£o de Uvas

Este exemplo conceitual demonstra como utilizar os dados da API para criar um modelo preditivo de machine learning:

#### Fluxo de Trabalho

1. **Coleta de Dados**: Utilize o endpoint `/producao/filter?categoria=VINHO_DE_MESA&produto=TINTO` para obter dados hist√≥ricos de produ√ß√£o de vinho tinto.

2. **Prepara√ß√£o dos Dados**: 
   - Converta os dados para formato num√©rico
   - Crie features adicionais como produ√ß√£o do ano anterior e varia√ß√£o percentual
   - Codifique vari√°veis categ√≥ricas
   - Divida os dados em conjuntos de treino e teste

3. **Treinamento do Modelo**:
   - Utilize um algoritmo como Random Forest Regressor
   - Treine o modelo com dados hist√≥ricos
   - Avalie o desempenho usando m√©tricas como MSE e R¬≤

4. **Visualiza√ß√£o e An√°lise**:
   - Identifique as features mais importantes para a previs√£o
   - Compare valores reais vs. previstos
   - Analise tend√™ncias por regi√£o e tipo de produto

5. **Previs√µes Futuras**:
   - Utilize o modelo treinado para prever a produ√ß√£o do pr√≥ximo ano
   - Gere previs√µes por estado e produto
   - Crie relat√≥rios para auxiliar no planejamento da produ√ß√£o

Este fluxo de trabalho permite que produtores e empresas do setor vitivin√≠cola utilizem os dados hist√≥ricos disponibilizados pela API para tomar decis√µes baseadas em dados e antecipar tend√™ncias de mercado.
''',
            "deployment_url": None
        },
        {
            "url": "https://github.com/RVerdiF/TechChallenge3",
            "title": "BTC Prediction Project",
            "readme": '''
Projeto de previs√£o de pre√ßo do Bitcoin usando Machine Learning com arquitetura modular e dashboard interativo.

## Estrutura do Projeto

```
/Tech_Challenge_3
‚îú‚îÄ‚îÄ .devcontainer/              # Configura√ß√µes do Dev Container
‚îú‚îÄ‚îÄ .git/                       # Reposit√≥rio Git
‚îú‚îÄ‚îÄ .venv/                      # Ambiente virtual Python
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ApiHandler/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_api.py         # Coleta de dados via yfinance
‚îÇ   ‚îú‚îÄ‚îÄ AuthHandler/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.py             # Gerenciamento de autentica√ß√£o de usu√°rios
‚îÇ   ‚îú‚îÄ‚îÄ BacktestHandler/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backtesting.py      # L√≥gica para backtesting de estrat√©gias
‚îÇ   ‚îú‚îÄ‚îÄ DataHandler/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_handler.py     # Gerenciamento do DB de pre√ßos (btc_prices.db)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_db_handler.py # Gerenciamento do DB de modelos (models.db)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py # Cria√ß√£o de features
‚îÇ   ‚îú‚îÄ‚îÄ LogHandler/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ log_config.py       # Configura√ß√£o de logs
‚îÇ   ‚îú‚îÄ‚îÄ ModelHandler/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_model.py      # Treinamento do modelo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predict.py          # Previs√µes
‚îÇ   ‚îú‚îÄ‚îÄ Orchestration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_project.py      # Script de configura√ß√£o inicial (obsoleto)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ update_scheduler.py # L√≥gica para atualiza√ß√£o di√°ria de dados
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Inicializador do pacote src
‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Configura√ß√µes do projeto (paths)
‚îú‚îÄ‚îÄ .gitignore                  # Arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ main.py                     # Interface Streamlit
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o
```

**Observa√ß√µes:**
- Os bancos de dados `btc_prices.db`, `users.db` e `models.db` s√£o criados dinamicamente no diret√≥rio `src/DataHandler/`.
- Os diret√≥rios `__pycache__` s√£o criados automaticamente pelo Python.

## Instala√ß√£o

1. Clone o reposit√≥rio
2. Crie e ative um ambiente virtual:
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate    # Windows
```
3. Instale as depend√™ncias:
```bash
pip install -r requirements.txt
```

## Uso

**Para iniciar o dashboard interativo:**
```bash
streamlit run main.py
```
A aplica√ß√£o ir√° iniciar e apresentar uma tela de login. Voc√™ pode criar um novo usu√°rio e, ap√≥s o login, o dashboard ser√° exibido. A primeira carga de dados √© feita automaticamente em segundo plano.

## Funcionalidades

### Coleta de Dados
- **Fonte**: Yahoo Finance (yfinance)
- **Atualiza√ß√£o Autom√°tica**: Os dados s√£o atualizados diariamente em segundo plano para incluir o dia mais recente.
- **Armazenamento**: SQLite local (`btc_prices.db`).

### Engenharia de Features
- **Indicadores T√©cnicos**: SMA, RSI, EMA, MACD, Bandas de Bollinger, Oscilador Estoc√°stico.
- **Target**: Classifica√ß√£o bin√°ria (Alta/Queda do pr√≥ximo dia).

### Modelo de Machine Learning
- **Algoritmo**: LightGBM Classifier
- **Valida√ß√£o**: Time Series Split (3 folds)
- **M√©tricas**: Accuracy e F1-Score
- **Armazenamento**: O modelo treinado, m√©tricas e par√¢metros s√£o salvos em um banco de dados SQLite, associados √† conta do usu√°rio.

### Dashboard Web
- **Autentica√ß√£o**: Sistema de login e registro de usu√°rios.
- **Atualiza√ß√£o Autom√°tica**: Os dados de pre√ßo s√£o atualizados diariamente em background.
- **Gr√°fico interativo**: Hist√≥rico de pre√ßos com Plotly, com sele√ß√£o de per√≠odo.
- **Previs√µes**: Bot√£o para gerar previs√£o do pr√≥ximo dia com a confian√ßa do modelo do usu√°rio.
- **Treinamento Customizado**: Interface para ajustar par√¢metros de features e do modelo, e treinar um novo modelo para o usu√°rio logado. Os par√¢metros s√£o salvos no banco de dados para futuras sess√µes.
- **Backtesting**: P√°gina para simular uma estrat√©gia de trading baseada no modelo treinado.

## Fluxos da Aplica√ß√£o

(Os diagramas de fluxo fornecem uma vis√£o geral da l√≥gica da aplica√ß√£o. Note que o armazenamento, antes baseado em arquivos, agora √© centralizado em bancos de dados SQLite.)

### Fluxo Principal
![Fluxo Principal da Aplica√ß√£o](imgs/Main%20Application%20Flow.jpg)

### Fluxo de Atualiza√ß√£o de Dados (Dashboard)
![Fluxo de Atualiza√ß√£o de Dados](imgs/Data%20Update%20Flow%20(Dashboard).jpg)

### Fluxo de Predi√ß√£o (Dashboard)
![Fluxo de Predi√ß√£o](imgs/Prediction%20Flow%20(Dashboard).jpg)

### Fluxo de Treinamento (Configura√ß√µes)
![Fluxo de Treinamento](imgs/Training%20Flow%20(Settings).jpg)

### Fluxo de Backtesting
![Fluxo de Backtesting](imgs/Backtesting%20Flow%20(Backtesting).jpg)

## Arquitetura Modular

### `main.py`
- Interface principal com Streamlit. Gerencia a navega√ß√£o, estado da sess√£o e inicializa√ß√£o de tarefas em segundo plano (atualiza√ß√£o de dados).

### `src/`
- **`ApiHandler/`**: Coleta de dados externos.
- **`AuthHandler/`**: Gerencia a autentica√ß√£o de usu√°rios em seu pr√≥prio banco de dados (`users.db`).
- **`BacktestHandler/`**: Cont√©m a l√≥gica para executar a simula√ß√£o de backtesting.
- **`DataHandler/`**:
  - `data_handler.py`: Gerencia o banco de dados de pre√ßos (`btc_prices.db`) e metadados (ex: data da √∫ltima atualiza√ß√£o).
  - `model_db_handler.py`: Gerencia o banco de dados de modelos e configura√ß√µes dos usu√°rios (`models.db`).
  - `feature_engineering.py`: Fun√ß√µes para calcular indicadores t√©cnicos.
- **`LogHandler/`**: Configura√ß√£o centralizada do logger.
- **`ModelHandler/`**:
  - `train_model.py`: Pipeline de treinamento que salva o modelo e m√©tricas no banco de dados via `model_db_handler`.
  - `predict.py`: Carrega um modelo do banco de dados para fazer previs√µes.
- **`Orchestration/`**:
  - `update_scheduler.py`: Cont√©m a l√≥gica da tarefa em segundo plano que verifica e dispara a atualiza√ß√£o di√°ria dos dados.

## Depend√™ncias

- **pandas**: Manipula√ß√£o de dados
- **yfinance**: API do Yahoo Finance
- **scikit-learn**: M√©tricas e valida√ß√£o
- **lightgbm**: Algoritmo de ML
- **joblib**: Serializa√ß√£o do modelo
- **streamlit**: Interface web
- **plotly**: Visualiza√ß√µes interativas

## Notas T√©cnicas

- **Multi-usu√°rio**: A arquitetura suporta m√∫ltiplos usu√°rios, onde cada um pode treinar, salvar e usar seu pr√≥prio modelo e configura√ß√µes, persistidos em um banco de dados.
- **Tarefas em Segundo Plano**: A atualiza√ß√£o di√°ria de dados √© executada em uma thread separada, garantindo que a interface do usu√°rio n√£o seja bloqueada.
''',
            "deployment_url": "https://techchallenge3rafaelfreitas.streamlit.app/"
        },
        {
            "url": "https://github.com/RVerdiF/PaysimViz",
            "title": "PaySim Dataset Explorer",
            "readme": '''
A Streamlit application for exploring and analyzing the PaySim synthetic financial dataset. This tool provides insights into transaction patterns, fraud detection, and data anomalies.

## Features

- **Efficient, High-Performance Backend:** Designed to handle the large PaySim dataset without crashing. The app uses a query-based architecture, parallel data loading, and the high-performance Polars data analysis library to ensure stability and speed.
- **Data-Aware Loading:** The app checks if the dataset exists. If not, it presents a simple one-click download button that securely uses your Kaggle credentials from Streamlit's secrets.
- **Home Page:** General statistics about the dataset, including null value analysis, negative value checks, and zero-amount transaction reports, all generated through efficient, direct-to-database queries.
- **Data Exploration:** In-depth analysis of the dataset, including:
    - Transaction distribution over time (Hourly, Daily, Weekly).
    - Transaction type overview (Pie chart and summary table).
    - Analysis of the `isFlaggedFraud` feature's performance.
    - Insights into transaction amounts and descriptive statistics.
    - "Mule account" identification and analysis, powered by Polars for high-performance aggregation.
    - Analysis of balance-draining fraudulent transactions.

## Architecture and Design

This application has been refactored for high performance and memory safety to handle the multi-gigabyte PaySim dataset.

- **SQL-Centric Backend:** Instead of loading the entire dataset into memory, the application leverages a **SQLite** database backend. Each chart and analysis on the UI is powered by a specific, targeted SQL query that only pulls the necessary aggregated data.
- **Parallel Execution:** To ensure a fast and responsive user experience, all independent database queries on a given page are executed in **parallel** using a `concurrent.futures.ThreadPoolExecutor`.
- **High-Performance Aggregation with Polars:** For complex aggregations that are not suitable for SQL (like the "Top Mule Accounts Overview"), the application uses the **Polars** library. It employs a memory-safe streaming strategy: data is queried from the database in chunks, processed, and aggregated without ever holding the complete intermediate dataset in memory.

## Dataset

This application uses the PaySim dataset from Kaggle. You can find more information here: [PaySim Synthetic Dataset](https://www.kaggle.com/datasets/ealaxi/paysim1/data)

## Getting Started

### 1. Prerequisites

- Python 3.8+
- A Kaggle account and an API token.

### 2. Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd KrakenInterview
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows, use `.venv\Scripts\activate`
    ```

3.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up Kaggle API Credentials:**
    - Create a file at `.streamlit/secrets.toml`.
    - Add your Kaggle username and API key to this file in the following format:
      ```toml
      [kaggle]
      username = "YOUR_KAGGLE_USERNAME"
      key = "YOUR_KAGGLE_KEY"
      ```

### 3. Usage

1.  **Run the Streamlit application:**
    ```bash
    streamlit run app.py
    ```

2.  **Download the Dataset:**
    - On the first launch, the app will display a "Download Dataset" button.
    - Click this button to automatically download the data from Kaggle and set up the local SQLite database. This process is memory-safe and handles the large CSV in chunks.
    - After the download is complete, the app will proceed to the main data explorer.

## Project Structure
```
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ app.py              # Main Streamlit application file
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml    # Streamlit secrets for Kaggle credentials
‚îú‚îÄ‚îÄ dl/                 # Directory for downloaded data
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ DataBase/       # Stores the final SQLite database
    ‚îú‚îÄ‚îÄ DataHandler/    # Modules for handling data (loading, downloading)
    ‚îú‚îÄ‚îÄ LogHandler/     # Module for logging setup
    ‚îú‚îÄ‚îÄ notebooks/      # Jupyter notebooks for exploration
    ‚îî‚îÄ‚îÄ Utils/          # Utility scripts (e.g., SQL queries)
```
''',
            "deployment_url": "https://rverdif-paysimviz-app-olphe6.streamlit.app/"
        },
    ]

    for project in projects_data:
        # Main title links to GitHub
        title_md = f"### [{project['title']}]({project['url']})"
        
        # Add deployment link if it exists
        if project.get("deployment_url"):
            title_md += f" | [Live App]({project['deployment_url']})"

        st.markdown(title_md, unsafe_allow_html=True)
        
        with st.expander("See details (README)"):
            st.markdown(project['readme'], unsafe_allow_html=True)
        st.markdown("---")
    
    st.markdown("##") # Add some space
    st.subheader("Key Professional Projects")
    st.markdown("""
    <div class="card">
        <p class="job-title">Autonomous AI Agents & MLOps Pipelines</p>
        <p style="font-style: italic; color: #a0a0a0;">A confidential project from my role as a Data Scientist.</p>
        <p>Designed, engineered, and orchestrated autonomous AI agents and robust MLOps pipelines to automate complex data workflows, enhancing system scalability and reliability.</p>
        <p><strong>Key Technologies:</strong> Python, AI/ML, MLOps, CI/CD, Docker, AWS</p>
    </div>
    <div class="card">
        <p class="job-title">Advanced Fraud Detection Algorithms</p>
        <p style="font-style: italic; color: #a0a0a0;">A confidential project from my role as a Data Scientist.</p>
        <p>Developed and implemented advanced algorithms for real-time transactional fraud prevention, significantly improving the integrity and security of payment systems.</p>
        <p><strong>Key Technologies:</strong> Python, Machine Learning, Predictive Modeling, Big Data (Spark/Kafka)</p>
    </div>
    <div class="card">
        <p class="job-title">Predictive Fraud Modeling</p>
        <p style="font-style: italic; color: #a0a0a0;">A confidential project from my role as a Data Scientist.</p>
        <p>Built and trained sophisticated predictive fraud models using Machine Learning and AI, enabling proactive responses to emerging and evolving security threats.</p>
        <p><strong>Key Technologies:</strong> Python, Scikit-learn, Deep Learning (TensorFlow/Keras), SQL</p>
    </div>
    <div class="card">
        <p class="job-title">Transaction Monitoring System</p>
        <p style="font-style: italic; color: #a0a0a0;">A confidential project from my role as a Data Analyst.</p>
        <p>Created and maintained a comprehensive transaction monitoring system. This involved developing a Python-based backend and dynamic Power BI dashboards to assess operational risk and optimize security.</p>
        <p><strong>Key Technologies:</strong> Python, Power BI, SQL, Statistical Analysis</p>
    </div>
    <div class="card">
        <p class="job-title">Data Governance & Automation in Snowflake</p>
        <p style="font-style: italic; color: #a0a0a0;">A confidential project from my role as a Data Analyst.</p>
        <p>Led data governance initiatives by creating and managing tables, views, and stored procedures in Snowflake. Automated key departmental processes, improving workflow efficiency and team productivity.</p>
        <p><strong>Key Technologies:</strong> Snowflake, Python, dbt, SQL, Data Governance</p>
    </div>
    """, unsafe_allow_html=True)


# --- TAB 3: PROFESSIONAL EXPERIENCE (ALL CONTENT RESTORED) ---
with tab3:
    st.header("Professional Experience")
    st.markdown("""
    <div class="card">
        <p class="job-title">Data Scientist, Fraud Prevention</p>
        <p class="company-name">Banco Mercantil do Brasil | November 2024 ‚Äì Present</p>
        <ul>
            <li>Engineer and orchestrate autonomous AI agents, designing and managing robust data and MLOps pipelines to automate complex workflows and enhance scalability.</li>
            <li>Develop and implement advanced algorithms for transactional fraud prevention, ensuring the integrity and security of payment systems.</li>
            <li>Build and train predictive fraud models using machine learning and AI, enabling proactive responses to emerging threats.</li>
            <li>Lead autonomous project management using DevOps practices, accelerating solution delivery and improving system reliability.</li>
        </ul>
    </div>
    <div class="card">
        <p class="job-title">Data Analyst, Fraud Prevention</p>
        <p class="company-name">Banco Mercantil do Brasil | January 2023 ‚Äì November 2024</p>
        <ul>
            <li>Monitored and analyzed key performance indicators (KPIs) to ensure the quality and effectiveness of the department's services.</li>
            <li>Developed and maintained a transaction monitoring system using Python and Power BI dashboards, applying statistical methods to assess operational risk and optimize security.</li>
            <li>Automated key departmental processes, significantly improving workflow efficiency and team productivity.</li>
            <li>Managed data governance by creating and maintaining tables, views, and procedures in Snowflake using Python, dbt, and SQL.</li>
        </ul>
    </div>
    <div class="card">
        <p class="job-title">Data Analysis & Fraud Prevention Assistant</p>
        <p class="company-name">Banco Mercantil do Brasil | December 2021 ‚Äì January 2023</p>
        <ul>
            <li>Provided critical support to the data analysis and fraud prevention teams, contributing to daily operations and strategic projects.</li>
            <li>Assisted in data preparation, cleaning, and preliminary analysis to support senior analysts and data scientists.</li>
        </ul>
    </div>
    <div class="card">
        <p class="job-title">Intern</p>
        <p class="company-name">Banco Mercantil do Brasil | November 2019 ‚Äì November 2021</p>
        <ul>
            <li>Gained foundational experience in the financial industry, supporting various teams with data-related tasks and process documentation.</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# --- TAB 4: EDUCATION & QUALIFICATIONS (ALL CONTENT RESTORED) ---
with tab4:
    st.header("Education")
    st.markdown("""
    <div class="card">
        <p class="degree-title">Postgraduate Specialization, Machine Learning Engineering</p>
        <p class="university-name">FIAP | Expected February 2025</p>
        <ul>
            <li><strong>Advanced Modeling:</strong> In-depth study of Classic Machine Learning and Deep Learning models, including supervised, unsupervised, and reinforcement learning.</li>
            <li><strong>Cloud & Big Data Ecosystems:</strong> Hands-on implementation of scalable ML solutions in cloud environments (AWS), leveraging platforms like Hadoop and Spark.</li>
            <li><strong>Specialized AI Applications:</strong> Covers advanced techniques in NLP, Computer Vision, and Generative AI models (GPT-4, Stable Diffusion).</li>
            <li><strong>MLOps & Productionalization:</strong> Emphasizes end-to-end MLOps practices, including automated data pipelines, containerization with Docker, and CI/CD for model deployment.</li>
        </ul>
    </div>
    <div class="card">
        <p class="degree-title">Postgraduate Specialization, Strategic Data Management & Analysis</p>
        <p class="university-name">PUC Minas | August 2022 - October 2023</p>
        <ul>
            <li><strong>Data-Driven Strategy:</strong> Provided knowledge of data-driven culture, data governance frameworks (LGPD/GDPR), and Agile Project Management.</li>
            <li><strong>Advanced Analytics & BI:</strong> Developed skills in advanced analytics using Python, including ETL/ELT processes and dimensional modeling for Data Warehouses.</li>
        </ul>
    </div>
    <div class="card">
        <p class="degree-title">Bachelor of Business Administration</p>
        <p class="university-name">PUC Minas | February 2018 ‚Äì December 2021</p>
        <ul>
            <li>Provided a strong foundation in strategic management, finance, marketing, and organizational processes.</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.header("Certifications & Licenses")
    st.markdown("""
    <div class="card">
        <ul>
            <li>Financial Markets | Yale University (2023)</li>
            <li>DBT & Snowflake (2023)</li>
            <li>Data Analysis and Power BI (2023)</li>
            <li>Data Analysis with Python (2022)</li>
            <li>Certified Yellow Belt, Lean Six Sigma (2022)</li>
            <li>Introduction to Data Science 2.0 (2020)</li>
            <li>Certified White Belt, Lean Six Sigma (2020)</li>
            <li>Transforming Ideas into Business (2015)</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)